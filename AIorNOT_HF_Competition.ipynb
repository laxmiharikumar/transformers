{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyN2W5PVXxeAiDPS0qOZw+20",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laxmiharikumar/transformers/blob/main/AIorNOT_HF_Competition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6QMcYbFvRck"
      },
      "outputs": [],
      "source": [
        "## Install required packages\n",
        "!pip install --upgrade huggingface_hub --quiet\n",
        "!pip install --upgrade datasets --quiet\n",
        "!pip install --upgrade transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from huggingface_hub import hf_hub_download"
      ],
      "metadata": {
        "id": "aJkJ9AZzvbyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login()"
      ],
      "metadata": {
        "id": "n-0Hcn1zvhol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data\n",
        "\n",
        "- Load the training and testing data sets"
      ],
      "metadata": {
        "id": "_G1vZra7vi2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset('competitions/aiornot')\n",
        "ds"
      ],
      "metadata": {
        "id": "988E_nO7vqhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ds[\"train\"]\n",
        "test_dataset = ds[\"test\"]"
      ],
      "metadata": {
        "id": "TahqMxe8wg6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train data has 0 and 1 labels\n",
        "import numpy as np\n",
        "unique, counts = np.unique(train_ds[\"label\"], return_counts=True)\n",
        "unique, counts"
      ],
      "metadata": {
        "id": "lBxMtSw40syp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test data all labels are set to -1\n",
        "unique, counts = np.unique(test_dataset[\"label\"], return_counts=True)\n",
        "unique, counts"
      ],
      "metadata": {
        "id": "x6FwWbEz1n7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect the Data\n",
        "- Randomly inspect the images"
      ],
      "metadata": {
        "id": "u6L-3zyGwupi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import io\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9MCCJHOfwt3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def visualize_random_images(tmp_ds1, tmp_ds2, m):\n",
        "def visualize_random_images(tmp_ds):\n",
        "  random_number = random.randint(0, len(tmp_ds)-1)\n",
        " \n",
        "  # plt.figure(figsize=(10,7))\n",
        "  plt.imshow(tmp_ds[random_number][\"image\"])\n",
        "  plt.axis(\"off\")\n",
        "  label_val = \"ai\" if tmp_ds[random_number][\"label\"] == 1 else \"no_ai\" if tmp_ds[random_number][\"label\"] == 0 else \"unknown\"\n",
        "  plt.title(str(random_number) + \" \" + label_val)\n",
        "  plt.show()\n",
        "\n",
        "  # # plt.figure(figsize=(10,7))\n",
        "  # plt.imshow(tmp_ds2[random_number][m])\n",
        "  # plt.axis(\"off\")\n",
        "  # label_val = \"ai\" if tmp_ds2[random_number][\"label\"] == 1 else \"no_ai\"\n",
        "  # plt.title(str(random_number) + \" \" + label_val)\n",
        "  # plt.show()"
      ],
      "metadata": {
        "id": "w1cOEISUw3qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly inspect images from training dataset\n",
        "visualize_random_images(train_ds)"
      ],
      "metadata": {
        "id": "IUvcwt2Bw_uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly inspect images from test dataset\n",
        "visualize_random_images(test_dataset)"
      ],
      "metadata": {
        "id": "zuWcThD1xe_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Split the training data into train and test sets for model building\n",
        "split_ds = train_ds.train_test_split(seed=42, shuffle=True, test_size=0.1)\n",
        "split_ds"
      ],
      "metadata": {
        "id": "baqqO0zD2VSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = split_ds[\"train\"]\n",
        "val_dataset = split_ds[\"test\"]"
      ],
      "metadata": {
        "id": "ZVKxZ5I24gEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(split_ds[\"train\"][\"label\"], return_counts=True)\n",
        "unique, counts"
      ],
      "metadata": {
        "id": "wZWYp45O22PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(split_ds[\"test\"][\"label\"], return_counts=True)\n",
        "unique, counts"
      ],
      "metadata": {
        "id": "0TnU2Q3Y3DG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the data\n",
        "\n",
        "- Normalize the data\n",
        "- Apply data augmentation"
      ],
      "metadata": {
        "id": "UmTlHRq13fFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
        "image_processor.size[\"height\"], image_processor.size[\"width\"] ## We should resize the images to this size"
      ],
      "metadata": {
        "id": "0dhElyXy3sWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
        "\n",
        "train_data_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Resizing(size[0], size[1]),         \n",
        "        tf.keras.layers.Rescaling(scale=1.0/255, offset=0),\n",
        "        tf.keras.layers.RandomRotation(factor=0.2),\n",
        "        tf.keras.layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
        "        tf.keras.layers.RandomFlip()       \n",
        "    ],\n",
        "    name=\"train_data_augmentation\",\n",
        ")\n",
        "\n",
        "\n",
        "# For the validation data and test data apply only Resizing and Rescaling\n",
        "val_data_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Resizing(size[0], size[1]),        \n",
        "        tf.keras.layers.Rescaling(scale=1.0/255, offset=0)\n",
        "    ],\n",
        "    name=\"val_data_augmentation\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "OcKLKuJt353E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def convert_to_tf_tensor(image: Image):\n",
        "    np_image = np.array(image)\n",
        "    tf_image = tf.convert_to_tensor(np_image)\n",
        "    # `expand_dims()` is used to add a batch dimension since\n",
        "    # the TF augmentation layers operates on batched inputs.\n",
        "    return tf.expand_dims(tf_image, 0)\n",
        "\n",
        "\n",
        "def preprocess_train(example_batch):\n",
        "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
        "    images = [\n",
        "        train_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
        "    ]\n",
        "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
        "    return example_batch\n",
        "\n",
        "\n",
        "def preprocess_val(example_batch):\n",
        "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
        "    images = [\n",
        "        val_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
        "    ]\n",
        "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]  \n",
        "    return example_batch"
      ],
      "metadata": {
        "id": "OW8S71nJ4Rsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Add a new column \"pixel_values\"\n",
        "new_column = [\"pixel_values\"] * len(train_dataset)\n",
        "train_dataset = train_dataset.add_column(\"pixel_values\", new_column)\n",
        "new_column = [\"pixel_values\"] * len(val_dataset)\n",
        "val_dataset = val_dataset.add_column(\"pixel_values\", new_column)\n",
        "new_column = [\"pixel_values\"] * len(test_dataset)\n",
        "test_dataset = test_dataset.add_column(\"pixel_values\", new_column)"
      ],
      "metadata": {
        "id": "f4qnyI5R_R29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Run data augmentations on training set, validation set and test set\n",
        "train_dataset.set_transform(preprocess_train)\n",
        "val_dataset.set_transform(preprocess_val)\n",
        "test_dataset.set_transform(preprocess_val)"
      ],
      "metadata": {
        "id": "ZiYyx4dt5OGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0][\"pixel_values\"].shape, val_dataset[0][\"pixel_values\"].shape, test_dataset[0][\"pixel_values\"].shape"
      ],
      "metadata": {
        "id": "A9VyzAEM72T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "aGg5Msp2_tq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As a final preprocessing step, create a batch of examples using DefaultDataCollator. \n",
        "# Unlike other data collators in 🤗 Transformers, the DefaultDataCollator does not apply additional preprocessing, such as padding.\n",
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "RTaxirgMCvjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model\n",
        "\n",
        "To fine-tune a model in TensorFlow, follow these steps:\n",
        "\n",
        "- Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n",
        "- Instantiate a pre-treined model.\n",
        "- Convert a 🤗 Dataset to a tf.data.Dataset.\n",
        "- Compile your model.\n",
        "- Add callbacks and use the fit() method to run the training.\n",
        "- Upload your model to 🤗 Hub to share with the community."
      ],
      "metadata": {
        "id": "Q_-RVNaE9Zqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 1 - Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n",
        "from transformers import create_optimizer\n",
        "\n",
        "batch_size = 32\n",
        "num_epochs = 3\n",
        "num_train_steps = len(train_dataset) * num_epochs\n",
        "learning_rate = 3e-5\n",
        "weight_decay_rate = 0.01\n",
        "\n",
        "optimizer, lr_schedule = create_optimizer(\n",
        "    init_lr=learning_rate,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=weight_decay_rate,\n",
        "    num_warmup_steps=0,\n",
        ")"
      ],
      "metadata": {
        "id": "dD31LXDG9mYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2 - Instantiate a pre-trained model\n",
        "from transformers import TFAutoModelForImageClassification\n",
        "\n",
        "label2id = {\"no_ai\": 0, \"ai\": 1}\n",
        "id2label = {0: \"no_ai\", 1: \"ai\"}\n",
        "\n",
        "pre_trained_model = TFAutoModelForImageClassification.from_pretrained(\n",
        "     checkpoint, \n",
        "     label2id=label2id,\n",
        "     id2label=id2label,\n",
        "     ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
        ")\n",
        "pre_trained_model.trainable = True"
      ],
      "metadata": {
        "id": "9mXM8W7j9xbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = train_dataset.remove_columns(\"image\")\n",
        "# val_dataset = val_dataset.remove_columns(\"image\")\n",
        "# test_dataset = test_dataset.remove_columns(\"image\")\n",
        "# train_dataset, val_dataset, "
      ],
      "metadata": {
        "id": "L0mDJO7n-tW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Add a Dense layer to the model\n",
        "# inputs = tf.keras.layers.Input(shape=(3, size[0], size[1]))\n",
        "# x = base_model(inputs)[0]\n",
        "# # x =  tf.keras.layers.Flatten()(x)\n",
        "# outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "# model = tf.keras.Model(inputs, outputs, name=\"my_laxs_first_model\")\n",
        "\n",
        "model_1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(3, size[0], size[1])),\n",
        "    pre_trained_model,\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "], name=\"model_1\")"
      ],
      "metadata": {
        "id": "Ms1a_GUSA5mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "id": "TyefqG2vBrh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.save(\"mmodel1\")"
      ],
      "metadata": {
        "id": "WASQR4rcII0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 3 - Convert a 🤗 Dataset to a tf.data.Dataset.\n",
        "!pip install evaluate --quiet"
      ],
      "metadata": {
        "id": "XkxVTTMyAfCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "wUzdLC0nDJaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "id": "DanO0rNCASws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate.module import Dataset\n",
        "# Convert your datasets to the tf.data.Dataset format using the to_tf_dataset and your data_collator:\n",
        "# converting our train dataset to tf.data.Dataset\n",
        "# Already shuffled while splitting\n",
        "tf_train_dataset = train_dataset.to_tf_dataset(\n",
        "    columns=[\"pixel_values\"], label_cols=[\"label\"], shuffle=False, batch_size=batch_size, collate_fn=data_collator \n",
        ")\n",
        "\n",
        "# converting our val dataset to tf.data.Dataset\n",
        "tf_eval_dataset = val_dataset.to_tf_dataset(\n",
        "    columns=[\"pixel_values\"], label_cols=[\"label\"], shuffle=False, batch_size=batch_size, collate_fn=data_collator \n",
        ")\n",
        "\n",
        "# converting our test dataset to tf.data.Dataset\n",
        "tf_test_dataset = test_dataset.to_tf_dataset(\n",
        "    columns=[\"pixel_values\"], label_cols=[\"label\"], shuffle=False, batch_size=batch_size, collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "OB1bv-wUCBG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_dataset"
      ],
      "metadata": {
        "id": "lnsatY0-ERsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 4 - Compile the model\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "model_1.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "Rvave6rJEFk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5 - Fit the model\n",
        "history_1 = model_1.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs)"
      ],
      "metadata": {
        "id": "MOpoyypNEO18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the validation and training data separately\n",
        "def plot_loss_curves(history):\n",
        "  \"\"\"\n",
        "  Returns separate loss curves for training and validation metrics.\n",
        "  \"\"\" \n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  accuracy = history.history['accuracy']\n",
        "  val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(len(history.history['loss']))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.plot(epochs, loss, label='training_loss')\n",
        "  plt.plot(epochs, val_loss, label='val_loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend();\n",
        "     \n"
      ],
      "metadata": {
        "id": "WWUPnjeXd0aF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy\n",
        "plot_loss_curves(history_1)"
      ],
      "metadata": {
        "id": "oK0qp2IRd36B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.evaluate(tf_eval_dataset)"
      ],
      "metadata": {
        "id": "L9qKwk7id_0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pred_probs = model_1.predict(tf_eval_dataset)\n",
        "model_pred_probs[:10]"
      ],
      "metadata": {
        "id": "4MIAcCa3fl7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_preds = tf.round(tf.squeeze(model_pred_probs))\n",
        "model_preds[:10]"
      ],
      "metadata": {
        "id": "i8cgAgYlf11Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_pred_probs = model_1.predict(tf_test_dataset)\n",
        "sub_pred_probs[:10]"
      ],
      "metadata": {
        "id": "mfJ4AAEBglTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_preds = tf.round(tf.squeeze(sub_pred_probs))\n",
        "sub_preds[:10]"
      ],
      "metadata": {
        "id": "P9hv_WbLg8kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sub_preds)"
      ],
      "metadata": {
        "id": "JZnFjNvU6aDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "log_loss(y_true, y_pred)"
      ],
      "metadata": {
        "id": "zOz0-7lvMYTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_test_dataset"
      ],
      "metadata": {
        "id": "9CgUBMTP5Mw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sub_file = hf_hub_download('competitions/aiornot', '.extras/sample_submission.csv', repo_type='dataset')"
      ],
      "metadata": {
        "id": "f7cljRqS8NN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(sample_sub_file)"
      ],
      "metadata": {
        "id": "S-eAGD3Ewq0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(sample_sub_file)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "0HrJOwyYL2rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "id": "7sno_riCPvgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_column = [\"foo\"] * len(test_dataset)\n",
        "p = test_dataset.add_column(\"predicted\", sub_preds.numpy())\n",
        "p"
      ],
      "metadata": {
        "id": "b3LM0jQ6P6gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(p)\n",
        "# p=p.remove_columns(\"image\")\n",
        "# p=p.remove_columns(\"pixel_values\")\n",
        "# p=p.remove_columns(\"label\")\n",
        "# p\n",
        "# # # , \"\""
      ],
      "metadata": {
        "id": "eaaMomxFsu4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "with open('protagonist.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for i in range(43442):\n",
        "      # m = p[i][\"id\"]\n",
        "      writer.writerow([p[i][\"id\"], p[i][\"predicted\"]])"
      ],
      "metadata": {
        "id": "Tnh6zzENM3Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('protagonist.csv')"
      ],
      "metadata": {
        "id": "lTE1AzWxZs4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8qeRMx3Zz61k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}